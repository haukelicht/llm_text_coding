{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, counting tokens, and cost calculations\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2024-03-25 |\n",
    "\n",
    "Although LLMs allow text-to-text user--computer interaction, behind the scenes the work with numbers.\n",
    "This means that any input text need to be converted into a sequence of integers (\"encoded\") that represent the words, subwords, and symbols in the input in a way the model can \"understand.\"\n",
    "This process of converted text inoputs into a sequence of integers is called *tokenization*.\n",
    "\n",
    "When we work with the OpenAI GPT models, you don't need to worry about this too much, since it handles the tokenization for you.\n",
    "The only reason we want to know about tokenization is to be able to count the number of tokens in your input text.\n",
    "Counting tokens is important because it helps you to compute the costs of using a language model and ensure that your input text is within the maximum token limit of the model you are using.\n",
    "\n",
    "In this notebook, we'll use the `tiktoken` python library to count the number of tokens in a given text.\n",
    "An alternative, interactive tool can be found https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "> The atomic unit of consumption for a language model is not a “word”, but rather a “token”.\n",
    "> You can kind of think of tokens as syllables, and on average they work out to about 750 words per 1,000 tokens.\n",
    "> They represent many concepts beyond just alphabetical characters – such as punctuation, sentence boundaries, and the end of a document.\n",
    "> &mdash; [source](https://github.com/brexhq/prompt-engineering?tab=readme-ov-file#tokens)\n",
    "\n",
    "Learn more about tokenizers and their reason of existence here: https://huggingface.co/docs/transformers/tokenizer_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token limits a.k.a. context window size\n",
    "\n",
    "LLMs are \"stateless\" and thus cannot remember anything about previous requests or converations.\n",
    "This means that so you always need to include everything that it might need to know that is specific to the current session.\n",
    "\n",
    "This is a major downside of LLMs, as it means that the leading language model architecture, the Transformer, has a fixed input and output size – at a certain point the prompt cannot grow any larger.\n",
    "\n",
    "The total size of the prompt, sometimes referred to as the **context window**, is model dependent.\n",
    "For GPT-3, it is 4,096 tokens. \n",
    "For GPT-4, it is 8,192 tokens or 32,768 tokens depending on which variant you use.\n",
    "\n",
    "You can find a detailed overview here: \n",
    "\n",
    "- for GPT-4 and its variants: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "- for GPT-3.5-turbo and its variants: https://platform.openai.com/docs/models/gpt-3-5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken==0.6.0\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tiktoken` makes available several encodings that are used by the varios OpenAI models, including GPT-3 and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list encoding names\n",
    "tiktoken.list_encoding_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, GPT-4 (snapshot from June 2023) uses the 'cl100k_base' encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cl100k_base'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the encoding model for the desired model\n",
    "encoding = tiktoken.encoding_for_model('gpt-4-0613')\n",
    "encoding.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `encoding` instance created above, you can tokenize and encode any text input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 11, 1917, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode('Hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are just token's indexes in the tokenizer's vocabulary. They are not the actual token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' world', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(tok).decode() for tok in encoding.encode('Hello, world!')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we can tokenize a text, counting the number of tokens is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = encoding.encode('Hello, world!')\n",
    "len(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Liberal Alliance er det eneste alternativ til  et træt VKO-flertal, som er bange for både  reformer, udlændinge og vælgere, og en  populistisk S/SF-regering, som er bange  for præcis de samme ting - og som vil indføre endnu flere skatter, afgifter, regler og  forbud,  end  den  nuværende  regering  plager os med.\"\n",
    "num_tokens_from_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more advanced approach\n",
    "\n",
    "If we use the function above, we need to reload the encoding every time we want to count the tokens in a new text.\n",
    "Also, we can only input one text at a time.\n",
    "\n",
    "To avoid this, we can create a class that loads the encoding once and then allows us to count the tokens in multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self, encoding_name: Union[str, None] = None, model: Union[str, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with either a model or an encoding name.\n",
    "\n",
    "        Args:\n",
    "            encoding_name (Union[str, None]): The name of the encoding to use. Default is None.\n",
    "            model (Union[str, None]): The model to use for encoding. Default is None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If neither model nor encoding_name is provided.\n",
    "            ValueError: If both model and encoding_name are provided.\n",
    "        \"\"\"\n",
    "        # ensure that either model or encoding_name is provided\n",
    "        if model is None and encoding_name is None:\n",
    "            raise ValueError(\"Either `model` or `encoding_name` must be provided.\")\n",
    "        if model is not None and encoding_name is not None:\n",
    "            raise ValueError(\"Only one of `model` or `encoding_name` can be provided.\")\n",
    "        if encoding_name:\n",
    "            self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        else:\n",
    "            self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Count the number of tokens in the input.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        if isinstance(input, str):\n",
    "            return len(self.encoding.encode(input))\n",
    "        else:\n",
    "            toks = self.encoding.encode_batch(input)\n",
    "            return [len(t) for t in toks]\n",
    "\n",
    "    def __call__(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Call the tokenizer on the input. This is equivalent to calling count_tokens.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        return self.count_tokens(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Note:_** This code defines a `TokenCounter` class that can be initialized with either a model or an encoding name. The `count_tokens` method counts the number of tokens in the input, and the `__call__` method allows the tokenizer to be called like a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCounter(model=\"gpt-4-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter([\"Hello, world!\", \"I'm tiktoken!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing API usage costs\n",
    "\n",
    "OpenAI charges model usage costs based on the number of tokens processed by the model.\n",
    "This means that you need to be aware of the number of tokens in your input text and the (expected) number of tokens in its response to avoid unexpected costs.\n",
    "\n",
    "To see what OpenAI charges you per 1,000,000 (one million) input and output tokens, see https://openai.com/pricing\n",
    "\n",
    "On March 25, 2024, the cost for using GPT-4 are: $30.00 per 1M input tokens, and $60.00 per 1M output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example calculations\n",
    "\n",
    "Say you have a dataset with ten sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I absolutely love this product. It's incredibly user-friendly.\",\n",
    "    \"I'm really disappointed with the service I received.\",\n",
    "    \"The weather today is absolutely beautiful, it makes me feel so happy.\",\n",
    "    \"I'm feeling really down today, nothing seems to be going right.\",\n",
    "    \"This is the best day of my life, I couldn't be happier!\",\n",
    "    \"I'm so frustrated with the lack of communication from the team.\",\n",
    "    \"The movie was a masterpiece, the storyline was captivating and the acting was superb.\",\n",
    "    \"I'm feeling really stressed about the upcoming exam.\",\n",
    "    \"The food at the restaurant was delicious, I'll definitely be going back.\",\n",
    "    \"I'm really angry about the decision, it's completely unfair.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And say your instructions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You will be provided with a sentence. \n",
    "\n",
    "Your task is to classify the sentence's sentiment as either positive, negative, or neutral.\n",
    "\n",
    "Please choose one of the following categories: positive, negative, neutral.\n",
    "\n",
    "Only respond with your chosen category and no further text or explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our label classes have the following numbers of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(['positive', 'negative', 'neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each sentence, we need to send the instructions plus the sentences as input and we will receive one of the three answer categories.\n",
    "So we can calulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of input tokens: 682\n",
      "# of output tokens: 10\n"
     ]
    }
   ],
   "source": [
    "n_input_tokens = sum(token_counter(dataset)) + len(dataset) * token_counter(instructions)\n",
    "n_output_tokens = len(dataset)\n",
    "\n",
    "print('# of input tokens:', n_input_tokens)\n",
    "print('# of output tokens:', n_output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can compute the cost (in U.S. $) for requesting classifications of the ten examples in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02106"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    n_input_tokens/1_000_000 * 30.00 # $30 per 1M input tokens\n",
    "    +\n",
    "    n_output_tokens/1_000_000 * 60.00 # $60 per 1M output tokens\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
