{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Limits to) reproducibility with OpenAI's models\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2024-03-25 |\n",
    "\n",
    "Generating chat completions with OpenAI's models is non-deterministic, meaning that model outputs may differ from request to request.\n",
    "This is problematic for using its and other closed-source models in research (cf. Palmer et al. [2024](https://doi.org/10.1038/s43588-023-00585-1)), because it hinders *reproducibility* &mdash; the ability to generate the reported results of a study again using the same data and methods.\n",
    "\n",
    "OpenAI has announced its measures to remedy this in November 2023 ([source](https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter)):\n",
    "\n",
    "1. user can set the `seed` parameter to an integer value in their request\n",
    "2. extract the \"system_fingerprint\" field from the response, which identifies the current combination of model weights, infrastructure, and other configuration options used by OpenAI servers.\n",
    "\n",
    "If all other parameters (prompt, temperature, top_p, etc.) are held constant across requests, one should obtain consistent responses across requests *if* the same system is used (see the fingerprint).\n",
    "\n",
    "> If the seed, request parameters, and system_fingerprint all match across your requests, then model outputs will mostly be identical\n",
    "\n",
    "The **problems**:\n",
    "\n",
    "- changes to the numerical configuration of OpenAI's infrastructure happen a few times a year &rArr; so as time passes, it gets increaseingly unlikely that you receive the same system fingerprint\n",
    "- its anyways unlikely that you get the same system for consecutive requests ü§∑‚Äç‚ôÇÔ∏è\n",
    "- even if you know the system fingerprint and distribute it with your replication materials, there is currently no way of passing the finger rpint when making an API request so that you'd use the same system as when making the original request ü§∑‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = 'gpt-4-turbo-preview'\n",
    "\n",
    "import pandas as pd\n",
    "# from tqdm.auto import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=42 # https://medium.com/geekculture/the-story-behind-random-seed-42-in-machine-learning-b838c4ac290a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's request an answer to a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_template=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, who won the Nobel Prize in Physics in 2021? Answer only with the name of the laureate.\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=chat_template, \n",
    "    max_tokens=20,\n",
    "    seed=SEED,\n",
    "    temperature=0.0,\n",
    "    top_p=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system fingerprint: fp_166a8e22c6\n",
      "response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi.\n"
     ]
    }
   ],
   "source": [
    "print('system fingerprint:', response.system_fingerprint)\n",
    "print('response:', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system fingerprint: fp_a7daf7c51e ; response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi.\n",
      "system fingerprint: fp_a7daf7c51e ; response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi.\n",
      "system fingerprint: fp_a7daf7c51e ; response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi\n",
      "system fingerprint: fp_8cc6edbbd5 ; response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi\n",
      "system fingerprint: fp_a7daf7c51e ; response: Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=chat_template, \n",
    "        max_tokens=20,\n",
    "        seed=SEED,\n",
    "        temperature=0.0,\n",
    "        top_p=0.0\n",
    "    )\n",
    "    print('system fingerprint:', response.system_fingerprint, '; response:', response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when we make the same request with exactly the same parameters, we still might be returned responses from different systems.\n",
    "The upside is that this doesn't affect the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = chat_template=[\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I loved the new Batman movie\"\n",
    "    },\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model='gpt-4', messages=chat_template)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
