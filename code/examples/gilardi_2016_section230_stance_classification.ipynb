{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4 turbo for Section 230 stance classification\n",
    "\n",
    "In this notebook, we take data analyzed in Gilardi et al. ([2023]()) to illustrate how to use GPT-4-turbo through the OpenAI chat completions API to classify stances in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/cm0nk6y92rz2l6ct3npgw9tr0000gn/T/ipykernel_4422/1905943439.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4-turbo-preview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self, encoding_name: Union[str, None] = None, model: Union[str, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with either a model or an encoding name.\n",
    "\n",
    "        Args:\n",
    "            encoding_name (Union[str, None]): The name of the encoding to use. Default is None.\n",
    "            model (Union[str, None]): The model to use for encoding. Default is None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If neither model nor encoding_name is provided.\n",
    "            ValueError: If both model and encoding_name are provided.\n",
    "        \"\"\"\n",
    "        # ensure that either model or encoding_name is provided\n",
    "        if model is None and encoding_name is None:\n",
    "            raise ValueError(\"Either `model` or `encoding_name` must be provided.\")\n",
    "        if model is not None and encoding_name is not None:\n",
    "            raise ValueError(\"Only one of `model` or `encoding_name` can be provided.\")\n",
    "        if encoding_name:\n",
    "            self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        else:\n",
    "            self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Count the number of tokens in the input.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        if isinstance(input, str):\n",
    "            return len(self.encoding.encode(input))\n",
    "        else:\n",
    "            toks = self.encoding.encode_batch(input)\n",
    "            return [len(t) for t in toks]\n",
    "\n",
    "    def __call__(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Call the tokenizer on the input. This is equivalent to calling count_tokens.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        return self.count_tokens(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCounter(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Your task is to read tweets about content moderation and classify what stance they take on Section 230 (if any).\n",
    "\n",
    "In the context of content moderation, Section 230 is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users. This means that if someone posts something illegal or harmful on a website, the website itself cannot be sued for allowing it to be posted. However, websites can still choose to moderate content and remove anything that violates their own policies. \n",
    "\n",
    "For each tweet in the sample, follow these instructions: \n",
    "\n",
    "1. Carefully read the text of the tweet, paying close attention to details.\n",
    "2. Classify the tweet as having a positive stance towards Section 230, a negative stance, or a neutral stance.\n",
    "\n",
    "For each tweet, choose one of the following categories: \"negative\", \"neutral\", \"positive\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00181"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter(instructions)/1000*0.01 # dollar cents per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = \"../../data/gilardi_chatgpt_2023/gilardi_chatgpt_2023_section230_stance.csv\"\n",
    "df = pd.read_csv(fp)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutral     420\n",
       "negative    327\n",
       "positive     33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative',\n",
       " 'Isnâ€™t it fascinating that my Twitter followers went from 1K to 23.9K in a matter of days and now are suddenly starting to decline over the last 48 hours? Big Tech censorship is a clear and present danger to America. Section 230 protection must go away one way or another. #USA')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0# \n",
    "df.label.values[i], df['text'].values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": df['text'].values[0]}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    seed=42,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "results = response.choices[0].message.content\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    messages = [ \n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    results = response.choices[0].message.content\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1316498227399720960</td>\n",
       "      <td>BigTech finally went too far censoring conserv...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1347788874890715138</td>\n",
       "      <td>A lot of big section 230 talk over the last ye...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1275813106212655104</td>\n",
       "      <td>.@Twitter is at it again â€” censoring @realDona...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1336723788759752704</td>\n",
       "      <td>.@Google/@YouTube, under false and non-sensica...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1340367091904491520</td>\n",
       "      <td>@zackfox This is why we need to abolish sectio...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1313573294516428804</td>\n",
       "      <td>@realDonaldTrump If you repeal section 230 of ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1348388767032242176</td>\n",
       "      <td>@generativist Section 230 literally exists to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1346763207344599041</td>\n",
       "      <td>$2,000 checks, monthly\\r\\nRestore net neutrali...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1266180983155494913</td>\n",
       "      <td>This EO is a reactionary and politicized appro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1300466976997990400</td>\n",
       "      <td>wow. i shouldn't be surprised by this, but I a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              status_id                                               text  \\\n",
       "0   1316498227399720960  BigTech finally went too far censoring conserv...   \n",
       "1   1347788874890715138  A lot of big section 230 talk over the last ye...   \n",
       "2   1275813106212655104  .@Twitter is at it again â€” censoring @realDona...   \n",
       "3   1336723788759752704  .@Google/@YouTube, under false and non-sensica...   \n",
       "4   1340367091904491520  @zackfox This is why we need to abolish sectio...   \n",
       "..                  ...                                                ...   \n",
       "70  1313573294516428804  @realDonaldTrump If you repeal section 230 of ...   \n",
       "71  1348388767032242176  @generativist Section 230 literally exists to ...   \n",
       "72  1346763207344599041  $2,000 checks, monthly\\r\\nRestore net neutrali...   \n",
       "73  1266180983155494913  This EO is a reactionary and politicized appro...   \n",
       "74  1300466976997990400  wow. i shouldn't be surprised by this, but I a...   \n",
       "\n",
       "       label  \n",
       "0   negative  \n",
       "1   negative  \n",
       "2   negative  \n",
       "3   negative  \n",
       "4   negative  \n",
       "..       ...  \n",
       "70  positive  \n",
       "71  positive  \n",
       "72  positive  \n",
       "73  positive  \n",
       "74  positive  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df.groupby('label').sample(25, random_state=42).reset_index(drop=True)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17465"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens in inputs\n",
    "n_tokens = samples.text.apply(token_counter.count_tokens).sum()\n",
    "# add token count for instructions\n",
    "n_tokens += token_counter(instructions) * len(samples)\n",
    "# add token count for outputs (multiplied by cost factor for output vs. input)\n",
    "n_tokens += len(samples) * 3\n",
    "\n",
    "# comopute cost (see https://openai.com/pricing)\n",
    "n_tokens/1000*0.01 # dollar cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify: apply custom classification function to all inputs\n",
    "results = samples.text.apply(classify_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.96      0.70        25\n",
      "     neutral       0.77      0.40      0.53        25\n",
      "    positive       0.89      0.64      0.74        25\n",
      "\n",
      "    accuracy                           0.67        75\n",
      "   macro avg       0.73      0.67      0.66        75\n",
      "weighted avg       0.73      0.67      0.66        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate: compute performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(samples.label, results.values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
