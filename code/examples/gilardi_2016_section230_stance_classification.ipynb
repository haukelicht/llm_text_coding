{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4 turbo for Section 230 stance classification\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2024-03-25 |\n",
    "\n",
    "In this notebook, we take data analyzed in Gilardi et al. ([2023](https://doi.org/10.1073/pnas.2305016120)) to illustrate how to use GPT-4-turbo through the OpenAI chat completions API to classify stances in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4-0125-preview'\n",
    "# note: if you do not have an OpenAI Plus subscription, use gpt-3.5-turbo instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TokenCounter:\n",
    "    def __init__(self, encoding_name: Union[str, None] = None, model: Union[str, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with either a model or an encoding name.\n",
    "\n",
    "        Args:\n",
    "            encoding_name (Union[str, None]): The name of the encoding to use. Default is None.\n",
    "            model (Union[str, None]): The model to use for encoding. Default is None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If neither model nor encoding_name is provided.\n",
    "            ValueError: If both model and encoding_name are provided.\n",
    "        \"\"\"\n",
    "        # ensure that either model or encoding_name is provided\n",
    "        if model is None and encoding_name is None:\n",
    "            raise ValueError(\"Either `model` or `encoding_name` must be provided.\")\n",
    "        if model is not None and encoding_name is not None:\n",
    "            raise ValueError(\"Only one of `model` or `encoding_name` can be provided.\")\n",
    "        if encoding_name:\n",
    "            self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        else:\n",
    "            self.encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    def count_tokens(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Count the number of tokens in the input.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        if isinstance(input, str):\n",
    "            return len(self.encoding.encode(input))\n",
    "        else:\n",
    "            toks = self.encoding.encode_batch(input)\n",
    "            return [len(t) for t in toks]\n",
    "\n",
    "    def __call__(self, input: Union[str, List[str]]) -> Union[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Call the tokenizer on the input. This is equivalent to calling count_tokens.\n",
    "\n",
    "        Args:\n",
    "            input (Union[str, List[str]]): The input to tokenize. Can be a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[int, List[int]]: The number of tokens in the input. If the input is a list, returns a list of token counts.\n",
    "        \"\"\"\n",
    "        return self.count_tokens(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCounter(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = os.path.join('..', '..')\n",
    "data_path = os.path.join(base_path, 'data', 'gilardi_chatgpt_2023') \n",
    "\n",
    "fp = os.path.join(data_path, \"gilardi_chatgpt_2023_section230_stance.csv\")\n",
    "df = pd.read_csv(fp)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutral     420\n",
       "negative    327\n",
       "positive     33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Isn’t it fascinating that my Twitter followers went from 1K to 23.9K in a matter of days and now are suddenly starting to decline over the last 48 hours? Big Tech censorship is a clear and present danger to America. Section 230 protection must go away one way or another. #USA\n",
      "LABEL: negative\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print('TEXT:', df.text.values[i])\n",
    "print('LABEL:', df.label.values[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Your task is to read tweets about content moderation and classify what stance they take on Section 230 (if any).\n",
    "\n",
    "In the context of content moderation, Section 230 is a law in the United States that protects websites and other online platforms from being held legally responsible for the content posted by their users. This means that if someone posts something illegal or harmful on a website, the website itself cannot be sued for allowing it to be posted. However, websites can still choose to moderate content and remove anything that violates their own policies. \n",
    "\n",
    "For each tweet in the sample, follow these instructions: \n",
    "\n",
    "1. Carefully read the text of the tweet, paying close attention to details.\n",
    "2. Classify the tweet as having a positive stance towards Section 230, a negative stance, or a neutral stance.\n",
    "\n",
    "For each tweet, choose one of the following categories: \"negative\", \"neutral\", \"positive\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": df.text.values[i]}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    seed=42,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "results = response.choices[0].message.content\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    messages = [ \n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    results = response.choices[0].message.content\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1316498227399720960</td>\n",
       "      <td>BigTech finally went too far censoring conserv...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1347788874890715138</td>\n",
       "      <td>A lot of big section 230 talk over the last ye...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1275813106212655104</td>\n",
       "      <td>.@Twitter is at it again — censoring @realDona...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1336723788759752704</td>\n",
       "      <td>.@Google/@YouTube, under false and non-sensica...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1340367091904491520</td>\n",
       "      <td>@zackfox This is why we need to abolish sectio...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1313573294516428804</td>\n",
       "      <td>@realDonaldTrump If you repeal section 230 of ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1348388767032242176</td>\n",
       "      <td>@generativist Section 230 literally exists to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1346763207344599041</td>\n",
       "      <td>$2,000 checks, monthly\\r\\nRestore net neutrali...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1266180983155494913</td>\n",
       "      <td>This EO is a reactionary and politicized appro...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1300466976997990400</td>\n",
       "      <td>wow. i shouldn't be surprised by this, but I a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              status_id                                               text  \\\n",
       "0   1316498227399720960  BigTech finally went too far censoring conserv...   \n",
       "1   1347788874890715138  A lot of big section 230 talk over the last ye...   \n",
       "2   1275813106212655104  .@Twitter is at it again — censoring @realDona...   \n",
       "3   1336723788759752704  .@Google/@YouTube, under false and non-sensica...   \n",
       "4   1340367091904491520  @zackfox This is why we need to abolish sectio...   \n",
       "..                  ...                                                ...   \n",
       "70  1313573294516428804  @realDonaldTrump If you repeal section 230 of ...   \n",
       "71  1348388767032242176  @generativist Section 230 literally exists to ...   \n",
       "72  1346763207344599041  $2,000 checks, monthly\\r\\nRestore net neutrali...   \n",
       "73  1266180983155494913  This EO is a reactionary and politicized appro...   \n",
       "74  1300466976997990400  wow. i shouldn't be surprised by this, but I a...   \n",
       "\n",
       "       label  \n",
       "0   negative  \n",
       "1   negative  \n",
       "2   negative  \n",
       "3   negative  \n",
       "4   negative  \n",
       "..       ...  \n",
       "70  positive  \n",
       "71  positive  \n",
       "72  positive  \n",
       "73  positive  \n",
       "74  positive  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = df.groupby('label').sample(25, random_state=42).reset_index(drop=True)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute costs\n",
    "\n",
    "see this notebook for details: https://github.com/haukelicht/llm_text_coding/blob/main/code/tokenization_and_costs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17465"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens in inputs\n",
    "n_input_tokens = samples.text.apply(token_counter.count_tokens).sum()\n",
    "# add token count for instructions\n",
    "n_input_tokens += token_counter(instructions) * len(samples)\n",
    "# add token count for outputs (multiplied by cost factor for output vs. input)\n",
    "n_output_tokens = len(samples)\n",
    "\n",
    "# comopute cost (see https://openai.com/pricing)\n",
    "n_input_tokens/1_000_000*10.00 + n_output_tokens/1_000_000*30.00 # dollar cents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classify all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39478d9d72644b6b0bd6615695348f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classify: apply custom classification function to all inputs\n",
    "results = samples.text.progress_apply(classify_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.96      0.70        25\n",
      "     neutral       0.77      0.40      0.53        25\n",
      "    positive       0.89      0.64      0.74        25\n",
      "\n",
      "    accuracy                           0.67        75\n",
      "   macro avg       0.73      0.67      0.66        75\n",
      "weighted avg       0.73      0.67      0.66        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate: compute performance metrics\n",
    "print(classification_report(samples.label, results.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a macro F1 score of only 0.66, the overall perfomance is meeger.\n",
    "The performance in the \"neutral\" category is espeically low.\n",
    "\n",
    "**_Note_:** Gilardi et al. report an accuracy of ~0.7 (see panel A of their Figure 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1344115549937217538</td>\n",
       "      <td>i don't know what makes me more depressed, tha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1354071003010273281</td>\n",
       "      <td>SO IT'S TRUE, THEY ARE A PUBLISHER AND GETTING...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1341856271213838336</td>\n",
       "      <td>@Chris2every @DebbieforFL Changing the names o...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1343440457972416513</td>\n",
       "      <td>@LindseyGrahamSC Section 230 Must Terminated t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1266302387280506882</td>\n",
       "      <td>Ideally, keep Section 230 &amp;amp; require Twitte...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1382143650889670661</td>\n",
       "      <td>It’s time we examine the need for Section 230 ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1344091130300919808</td>\n",
       "      <td>#BREAKING: Senate Majority Leader Mitch McConn...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1316518172456030208</td>\n",
       "      <td>Big Tech claims they aren’t biased against Con...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1346850718674771968</td>\n",
       "      <td>STATEMENT: if Democrats control the Senate the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id                                               text  \\\n",
       "7  1344115549937217538  i don't know what makes me more depressed, tha...   \n",
       "1  1354071003010273281  SO IT'S TRUE, THEY ARE A PUBLISHER AND GETTING...   \n",
       "5  1341856271213838336  @Chris2every @DebbieforFL Changing the names o...   \n",
       "0  1343440457972416513  @LindseyGrahamSC Section 230 Must Terminated t...   \n",
       "8  1266302387280506882  Ideally, keep Section 230 &amp; require Twitte...   \n",
       "2  1382143650889670661  It’s time we examine the need for Section 230 ...   \n",
       "4  1344091130300919808  #BREAKING: Senate Majority Leader Mitch McConn...   \n",
       "3  1316518172456030208  Big Tech claims they aren’t biased against Con...   \n",
       "6  1346850718674771968  STATEMENT: if Democrats control the Senate the...   \n",
       "\n",
       "      label  \n",
       "7  positive  \n",
       "1  negative  \n",
       "5   neutral  \n",
       "0  negative  \n",
       "8  positive  \n",
       "2  negative  \n",
       "4   neutral  \n",
       "3   neutral  \n",
       "6  positive  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = df[~df.status_id.isin(samples.status_id)].groupby('label').sample(3, random_state=42)[[\"status_id\", \"text\", \"label\"]].reset_index(drop=True)\n",
    "# resuffle\n",
    "examples = examples.sample(frac=1.0, random_state=42)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, examples: pd.DataFrame):\n",
    "    messages = [{\"role\": \"system\", \"content\": instructions}]\n",
    "    \n",
    "    for _, d in examples.iterrows():\n",
    "        messages +=  [   \n",
    "            {\"role\": \"user\", \"content\": d.text},\n",
    "            {\"role\": \"assistant\", \"content\": d.label}\n",
    "        ]\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": text})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        seed=42,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    results = response.choices[0].message.content\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a7453f39f94d06901f577719df33b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# classify: apply custom classification function to all inputs\n",
    "results_fs = samples.text.progress_apply(classify_text, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "neutral     29\n",
       "negative    24\n",
       "positive    22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.76      0.78        25\n",
      "     neutral       0.62      0.72      0.67        25\n",
      "    positive       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.73        75\n",
      "   macro avg       0.74      0.73      0.74        75\n",
      "weighted avg       0.74      0.73      0.74        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(samples.label, results_fs.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_f1 = f1_score(samples.label, results.values, average='macro')\n",
    "fewshot_f1 = f1_score(samples.label, results_fs.values, average='macro')\n",
    "(round(fewshot_f1/zeroshot_f1, 3)-1)*100 # percentage improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
