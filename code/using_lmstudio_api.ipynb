{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LM Studio to request chat completions from an open-source LLM running locally\n",
    "\n",
    "| Authors | Last update |\n",
    "|:------ |:----------- |\n",
    "| Hauke Licht (https://github.com/haukelicht) | 2024-03-25 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST!!!\n",
    "\n",
    "\n",
    "1. start *LM Studio*\n",
    "2. if not yet done, download the \"TheBloke/zephyr_7B_beta_Q4_S_K.guff\" model (about 4GB on disk)\n",
    "3. head to the Local Server tab (`<->` on the left of the app window)\n",
    "4. load the zephyr model by choosing it from the dropdown (at the top of the app window)\n",
    "5. start the server by clicking on the green \"Start Server\" button\n",
    "7. setup upd and use the API with openai as shown below ðŸ‘‡\n",
    "\n",
    "source: https://lmstudio.ai/docs/local-server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the setup, the workflow is _identical_ to using OpenAI's models:\n",
    "\n",
    "1. Define the instructions.\n",
    "2. Construct the conversation historym, using the instuctions as system message and a to-be-classified text aas user input.\n",
    "3. Pass the conversation history to the client's chat completions method.\n",
    "4. Extract the response message from the completion.\n",
    "\n",
    "Let's reuse the tweet sentiment classification example from openAI ([source](https://platform.openai.com/examples/default-tweet-classifier)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"\n",
    "You will be provided with the text of a tweet. The text of the tweet will appear in the user input inside triple quotes. \n",
    "\n",
    "Your task is to classify the tweet's sentiment. Pleas chx^oose as your reply one of the following categories: positive, neutral, negative.\n",
    "\n",
    "Please only respond with your classification and no further text or explanations.\n",
    "\"\"\"\n",
    "\n",
    "example = '\"\"\"I loved the new Batman movie!\"\"\"'\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model='lm-studio', \n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": example}\n",
    "  ],\n",
    "  temperature=0.0,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_text_annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
